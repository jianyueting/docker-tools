<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://localhost:5432/hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>postgres</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value/>
  </property>
  <!-- 解决使用外部元数据管理启动报错 -->
  <property>
    <name>hive.metastore.schema.verification</name>
    <value>true</value>
  </property>

  <!-- 使用spark -->
  <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
  </property>
  <property>
    <name>hive.enable.spark.execution.engine</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.home</name>
    <value>/spark</value>
  </property>
  <property>
    <name>spark.master</name>
    <value>yarn</value>
  </property>
  <property>
    <name>spark.submit.deployMode</name>
    <value>cluster</value>
  </property>
  <property>
    <name>spark.enentLog.enabled</name>
    <value>true</value>
  </property>
  
  <!-- update支持 -->
  <property>
      <name>hive.txn.manager</name>
      <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
  </property>
  <property>
      <name>hive.support.concurrency</name>
      <value>true</value>
  </property>

  <!-- 事先用命令 hadoop fs -mkdir /spark-log 创建指定目录 -->
  <property>
      <name>spark.master</name>
      <value>spark://hadoop-namenode:7077</value>
  </property>
  <property>
    <name>spark.enentLog.dir</name>
    <value>hdfs://hadoop-namenode:9000/spark-log</value>
  </property>
  <property>
    <name>spark.serializer</name>
  <value>org.apache.spark.serializer.KryoSerializer</value>
  </property>
  <property>
    <name>spark.executor.memory</name>
    <value>512m</value>
  </property>
  <property>
    <name>spark.driver.memory</name>
    <value>512m</value>
  </property>
  <property>
    <name>spark.executor.extraJavaOptions</name>
    <value>-XX:+PrintGCDetails</value>
  </property>

  <!-- 登录验证 -->
  <property>
    <name>hive.server2.authentication</name>
    <value>CUSTOM</value>
  </property>
  <property>
    <name>hive.server2.custom.authentication.class</name>
    <value> org.apache.hadoop.hive.contrib.auth.CustomPasswdAuthenticator</value>
  </property>
  <property>
    <name>hive.jdbc_passwd.auth.hadoop</name>
    <value>123456</value>
  </property>
</configuration> 